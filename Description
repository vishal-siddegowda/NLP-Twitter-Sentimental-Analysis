Naive Bayes is a classification algorithm for binary (two-class) and multiclass classification problems. 
It is called Naive Bayes or idiot Bayes because the calculations of the probabilities for each class are simplified to make their calculations tractable.
Rather than attempting to calculate the probabilities of each attribute value, they are assumed to be conditionally independent given the class value.
This is a very strong assumption that is most unlikely in real data, 
i.e. that the attributes do not interact. Nevertheless, the approach performs surprisingly well on data where this assumption does not hold.


Task #1: Understand the Problem Statement and business case

Task #2: Import libraries and datasets

Task #3: Perform Exploratory Data Analysis

Task #4: Plot the word cloud

Task #5: Perform data cleaning - removing punctuation

Task #6: Perform data cleaning - remove stop words

Task #7: Perform Count Vectorization (Tokenization)

Task #8: Create a pipeline to remove stop-words, punctuation, and perform tokenization

Task #9: Understand the theory and intuition behind Naive Bayes classifiers

Task #10: Train a Naive Bayes Classifier

Task #11: Assess trained model performance

Task #12: Upsample the data

Task #13: Train and asses the model using upsamples data


Evaluation Metrics

Evaluation Criteria

Confusion Matrix:
Basically confusion matrix are the evaluation metrics which consists of information regarding actual and predicted class that are generated by the classification model. 
Usually ‘n’ represents the number of target classes in the dataset, n*n would be the size of the confusion matrix.
 
True Positive (TP): These are the instances where prediction is true, which represents that the normal frames have been effectively detected.
True Negative (TN): These are the instances where prediction is true, which depicts that the abnormal instances have been successfully identified.
False Positive (FP): These are the cases where prediction is false, that means the model has incorrectly identified the frames as normal.
False Negative (FN): These are the scenes where prediction is false, which means that the system has incorrectly detected the frames as abnormal.
      
Recall Score: 
It is a kind of metrics used to evaluate the performance of the system using true positive (TP) and false negative (FN) criteria. Thus, out of all the actual crowded scenes, recall tells us that how many normal/abnormal scenes were correctly identified.
			Recale: ((TP))/((TP+FN))

Precison score:
True negative (TN) and false positive (FP)  cases are being used to check the ability of model to classify the normal and abnormal cases. Precision tells us how many normal/abnormal cases were correctly labeled out of all the scenes that were actually normal/abnormal.
			Precision:  ((TN))/((TN+FP))
      
F1 Score/ F-measure: 
It is a metrics used to measure the models accuracy in terms of precision and recall. F1-score plays a vital role when precision and recall are of equal importance. Instead of trying to improve recall and precision independently, we can aim to achieve a good f1-score which inturn shows precision and recall have a better score.
			F1-score: 2 ((PRECISON*RECALL))/((PRECISION+RECALL))
